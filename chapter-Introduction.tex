%!TEX root = ComputerScienceOne.tex

%%Chapter: Introduction

Computers are awesome.  The human race has seen more advancements in the 
last 50 years than in the entire 10,000 years of human history.  Technology has
transformed the way we live our daily lives, how we interact with each other, and
has changed the course of our history.  Today, everyone carries smart phones
which have more computational power than supercomputers from 20 years ago.
Computing has become ubiquitous, the ``internet of things'' will soon become
a reality in which every device will become interconnected and data will be collected
and available even about the smallest of minutiae.

However, computers are also dumb.  Despite the most fantastical of depictions
in science fiction and and hopes of Artificial Intelligence, computers can only do 
what they are told to do.  The fundamental art of Computer Science is problem 
solving.  Computers are not good at problem solving; \emph{you} are the problem 
solver.  It is still up to you, the user, to approach a complex problem, study it, 
understand it, and develop a solution to it.  Computers are only good at automating 
solutions once you have solved the problem.

Computational sciences have become a fundamental tool of almost every
discipline.  Scholars have used textual analysis and data mining techniques to
analyze classical literature and historic texts, providing new insights and opening
new areas of study.  Astrophysicists have used computational analysis to
detect dozens of new exoplanets.  Complex visualizations and models can 
predict astronomical collisions on a galactic scale.  Physicists have used big 
data analytics to push the boundaries of our understanding of matter in the 
search for the Higgs boson and study of elementary particles.  Chemists 
simulate the interaction of millions of combinations of compounds without
the need for expensive and time consuming physical experiments.  
Biologists use massively distributed computing models to simulate 
protein folding and other complex processes.  Meteorologists can 
predict weather and climactic changes with ever greater accuracy.

Technology and data analytics have changed how political campaigns
are run, how products are marketed and even delivered.
Social networks can be data mined to track and predict the spread of flu
epidemics.  Computing and automation will only continue to grow.
The time is soon coming where basic computational thinking and the 
ability to develop software will be considered a basic skill necessary 
to every discipline, a requirement for many jobs and an 
essential skill akin to arithmetic.

Computer Science is not programming.  Programming is a necessary skill, but
it is only the beginning.  This book is intended to get you started
on your journey.

\section{Problem Solving}

At its heart, Computer Science is about problem solving.  Not problem solving
as a discipline, it would be hubris to think that Computer Science holds a 
monopoly on ``problem solving.''  Indeed, it would be hard to find any 
discipline in which solving problems was not a substantial aspect or 
motivation if not integral.  Instead, Computer Science is the study of
computers and computation.  It involves studying and understanding 
computational processes and the development of algorithms and techniques 
and how they apply to problems.

Problem solving skills are not something that can be distilled down into 
a single step-by-step process.  Each area and each problem comes with
its own unique challenges and considerations.  General problem solving 
techniques can identified, studied and taught, but problem solving skills 
are something that come with experience, hard work, and most importantly, 
failure.  Problem solving is part and parcel of the human experience.  

That doesn't mean we can't identify techniques and strategies for
approaching problems, in particular problems that lend themselves to
computational solutions.  A prerequisite to solving a problem is 
\emph{understanding} it.  What is the problem?  Who or what entities
are involved in the problem?  How do those entities interact with each
other?  What are the problems or deficiencies that need to be addressed?
Answering these questions, we get an idea of \emph{where we are}.

Ultimately, what is desired in a solution?  What are the objectives that
need to be achieved?  What would an ideal solution look like or what
would it do?  Who would use the solution and how would they use it?
By answering these questions, we get an idea of \emph{where we want to be}.

Once we know where we are and where we want to be, the problem solving
process can begin: how do we get from point $A$ to point $B$?

One of the first things a good engineer asks is: does a solution already exist?
If a solution already exists, then the problem is already solved!  Ideally the
solution is an ``off-the-shelf'' solution: something that already exists 
and which may have been designed for a different purpose but that can
be \emph{repurposed} for our problem.  However, there may be exceptions
to this.  The existing solution may be infeasible: it may be too resource
intensive or expensive.  It may be too difficult or too expensive to adapt
to our problem.  It may solve most of our problem, but may not work in
some corner cases.  It may need to be heavily modified in order to
work.  Still, this basic question may save a lot of time and effort in many
cases.  

In a very broad sense, the problem solving process is one that involves
\begin{enumerate}
  \item Design
  \item Implementation
  \item Testing
  \item Refinement
\end{enumerate}

After one has a good understanding of a problem, they can start designing
a solution.  A design is simply a plan on the construction of a solution.  A
design ``on paper'' allows you to see what the potential solution would look
like before investing the resources in building it.  It also allows you to identify
possible impediments or problems that were not readily apparent.  A
design allows you to an opportunity to think through possible alternative
solutions and weigh the advantages and disadvantages of each.  
Designing a solution also allows you to understand the problem better.

Design can involve gathering requirements and developing use cases.  
How would an individual \emph{use} the proposed solution?  What features
would they need or want?  

Implementations can involve building prototype solutions to test the
feasibility of the design.  It can involve building individual components and
integrating them together.

Testing involves finding, designing, and developing test cases: actual
instances of the problem that can be used to test your solution.  Ideally, the
a test case instance involves not only the ``input'' of the problem, but also
the ``output'' of the problem: a feasible or optimal solution that is known
to be correct via other means.  Test cases allow us to test our solution to
see if it gives correct and perhaps optimal solutions.  

Refinement is a process by which we can redesign, reimplement and
retest our solution.  We may want to make the solution more efficient, 
cheaper, simpler or more elegant.  We may find there are components
that are redundant or unnecessary and try to eliminate them.  We
may find errors or bugs in our solution that fail to solve the problem 
for some or many instances.  We may have misinterpreted requirements
or there may have been miscommunication, misunderstanding or
differing expectations in the solution between the designers and 
stakeholders.  Situations may change or requirements may have
been modified or new requirements created and the solution needs
to be adapted.  Each of these steps may need to be repeated many
times until an ideal solution, or at least acceptable, solution is achieved.

Yet another phase of problem solving is maintenance.  The solution
we create may need to be maintained in order to remain functional
and stay relevant.  Design flaws or bugs may become apparent that
were missed in the process.  The solution may need to be updated
to adapt to new technology or requirements.  

In software design there are two general techniques for problem solving;
top-down and bottom-up design.  A \gls{top-down design} strategy approaches a problem
by breaking it down into smaller and smaller problems until either a solution
is obvious or trivial or a preexisting solution (the aforementioned ``off-the-shelf'' 
solution) exists.  The solutions to the subproblems are combined and
interact to solve the overall problem.

A bottom-up strategy attempts to first completely define the smallest
components or entities that make up a system first.  Once these have
been defined and implemented, they are combined and interactions
between them are defined to produce a more complex system.

\section{Computing Basics}

Everyone has some level of familiarity with computers and computing 
devices just as everyone has familiarity with automotive basics.  However, 
just because you drive a car everyday doesn't mean you can tell the 
difference between a crankshaft and a piston.  To start, let's familiarize
ourselves with some basic concepts.

A computer is a device, usually electronic, that stores, receives, 
processes, and outputs information.  Modern computing devices include
everything from simple sensors to mobile devices, tablets, desktops, 
mainframes/servers, supercomputers, to huge grid clusters consisting
of multiple computers networked together.

Computer hardware usually refers to the physical components in a 
computing system which includes input devices such as a mouse/touchpad, 
keyboard, or touchscreen, output devices such as monitors, storage 
devices such as hard disks and solid state drives, as well as the
electronic components such as graphics cards, main memory, motherboards and
chips that make up the \gls{cpuLabel}.

Computer processors are complex electronic circuits (referred to as
\gls{vlsiLabel}) which contain thousands of 
microscopic electronic transistors--electronic ``gates'' that can perform
logical operations and complex instructions.  In addition to the \gls{cpuLabel}
a processor may contain an \gls{aluLabel} that performs arithmetic 
operations such as addition, multiplication, division, etc.

Computer Software usually refers to the actual machine instructions
that are run on a processor.  Software is usually written in a high-level
programming language such as C or Java and then converted to 
machine code that the processor can execute.

Computers ``speak'' in binary code.  Binary is nothing more than a 
structured collection of 0s and 1s.  A single 0 or 1 is referred to as a \gls{bit}.  
Bits can be collected to form larger chunks of information: 8 bits form 
a \gls{byte}, 1024 bytes is referred to as a kilobyte, etc.  Table 
\ref{table:memoryUnits} contains a several more binary units.
Each unit is in terms of a power of 2 instead of a power of 10.  
As humans, we are more familiar with decimal--base-10 numbers
and so units are usually expressed as powers of 10, kilo- refers
to $10^3$, mega- is $10^6$, etc.  However, since binary is base-2
(0 or 1), units are associated with the closest power of 2.

\begin{table}
\centering
\begin{tabular}{l|l|r}
Unit & $2^n$ & Number of bytes \\
\hline\hline
Kilobyte (KB) & $2^{10}$ & 1,024 \\
Megabyte (MB) & $2^{20}$ & 1,048,576 \\
Gigabyte (GB) & $2^{30}$ & 1,073,741,824 \\
Terabyte (TB) & $2^{40}$ & 1,099,511,627,776 \\
Petabyte (PB) & $2^{50}$ & 1,125,899,906,842,624 \\
Exabyte (EB) & $2^{60}$ &  1,152,921,504,606,846,976\\
Zettabyte (ZB) & $2^{70}$ &  1,180,591,620,717,411,303,424\\
Yottabyte (YB) & $2^{80}$ &  1,208,925,819,614,629,174,706,176\\
\end{tabular}
\caption{Various units of digital information with respect to bytes.}
\label{table:memoryUnits}
\end{table}

Computers are binary machines because it is the most practical to
implement in electronic devices.  0s and 1s can be easily represented
by low/high voltage; low/high frequency; on-off; etc.  It is much easier
to design and implement systems that switch between only two states.

Computer \emph{memory} can refer to \emph{secondary memory}
which are typically longterm storage devices such as hard disks, flash 
drives, SD cards, optical disks (CDs, DVDs), etc. These generally have
large capacity but are slower (the time it takes to 
access a chunk of data is longer).  Or, it can refer to \emph{main memory}
(or primary memory): data stored on chips that is much faster but also
more expensive and thus generally smaller.

The first hard disk (IBM 350) was developed in 1956 by IBM and had a capacity of
3.75MB and cost \$3,200 (\$27,500 in 2015 dollars) per month to lease.
For perspective, the first commercially available TB hard drive was released in 2007.
As of 2015, terabyte hard disks can be commonly purchased for $50-$100.  

Main memory, sometimes referred to as \gls{ramLabel} consists of a collection
of \emph{addresses} along with \emph{contents}.  An address usually refers
to a single byte of memory.  The content, that is the byte of data that is stored at an address,
can be anything.  It can represent a number, a letter, etc.  In the end, to the computer
its all just a bunch of 0s and 1s.  For convenience, data, in particular memory
addresses are represented using hexadecimal, which is a base-16 counting
system using the symbols $0, 1, \ldots, 9, a, b, c, d, e, f$.  Numbers
are prefixed with a \texttt{0x} to indicate they represent hexadecimal numbers.

\begin{table}
\centering
\input{figures/figureMemory}
\caption[Depiction of Computer Memory]{Depiction of Computer Memory.  Each address 
refers to a byte, but different types of data (integers, floating-point numbers, characters) take
different amounts of memory.  Memory addresses and some data is represented in 
\emph{hexadecimal}.}
\end{table}

Separate computing devices can be connected to each other through
a \emph{network}.  Networks can be wired which provide large bandwidth 
(the amount of data that can be sent at any one time), but expensive to
build and maintain.  They can also be wireless, but provide shorter range
and lower bandwidth.  

\section{Basic Program Structure}

Programs start out as \emph{source code}, a collection of instructions usually written
in a high-level programming language.  A source file containing source code is nothing
more than a plain text file that can be edited by any text editor.  However, many 
developers and programmers utilize modern \gls{ideLabel} \index{Integrated Development Environment}
that provide a text editor with \emph{code highlighting}: various elements are 
displayed in various colors to make the code more readable and various elements
can be easily identified.  Mistakes such as unclosed comments or curly brackets
can be readily apparent with such editors.  IDEs can also provide automated 
compile/build features and other tools that make the development process easier
and faster.

Some languages are \emph{compiled} languages meaning that a source file
must be translated into the machine code that a processor can understand and
execute.  This is actually a multistep process.  A compiler may first preprocess the
source file(s) and perform some pre-compiler operations.  It may then transform
the source code into another language such as an assembly language, a lower
level, more machine-like language.  Ultimately, the compiler transforms the source
code into object code, a binary format that the machine can understand.  

To produce an executable file that can actually be run, a \emph{linker} may then
take the object code and link in any other necessary objects or precompiled
library code necessary to produce a final program.  Finally, an executable
file (still just a bunch of binary code) is produced.

We can now execute the program.  When a program is executed, a request
is sent to the operating system to load and run the program.  The operating 
system loads the executable file into memory and may setup additional memory
for its variables as well as its \emph{call stack} (memory to enable the program
to make function calls).  Once loaded and setup, the operating system begins
executing the instructions at the program's entry point.  

In many languages, a program's entry point is defined by a \emph{main} function
or method.  A program may contain many functions and pieces of code, but this
special function is defined as the one that gets called when a program starts.  
Without a main function, the code may still be useful: libraries contain many 
useful functions and procedures so that you don't have to write a program 
from scratch.  However, these functions are not intended to be run by themselves.
Instead, they are written so that other programs can use them.  A program
becomes executable when a main entry point is provided.  

This compile-link-execute process is roughly depicted in Figure \ref{figure:compilingProcess}.  An 
example of a simple C program can be found in Figure \ref{code:c:squareRoot}
along with the resulting assembly code produced by a compiler (gcc) in Figure
\ref{code:c:squareRootAssembly} and the final machine code represented
in hexadecimal in Figure \ref{code:c:squareRootMachine}.

\begin{figure}
\centering
\input{figures/figureCompileProcess}
\caption{A Compiling Process}
\label{figure:compilingProcess}
\end{figure}

\begin{listing}
\inputminted{c}{figures/squareRoot/squareRoot.c}
\caption{A simple program in C}
\label{code:c:squareRoot}
\end{listing}

\begin{listing}
\centering
\inputminted[fontsize=\tiny]{text}{figures/squareRoot/squareRoot.s}
\caption{A simple program in C, compiled to assembly}
\label{code:c:squareRootAssembly}
\end{listing}

\begin{listing}
\centering
\inputminted[fontsize=\scriptsize]{text}{figures/squareRoot/squareRoot.partial.txt}
\caption{A simple program in C, resulting machine code formatted in hexadecimal (partial)}
\label{code:c:squareRootMachine}
\end{listing}


In contrast, some languages are \emph{interpreted}, 
not compiled.  Instead, the source code is contained in a file usually referred
to as a \emph{script}.  Rather than being run directly by an operating system, 
the operating system loads and execute another program called an \emph{interpreter}.
The interpreter then loads the script, parses, and execute its instructions.
Interpreted languages may still have a predefined main function, but in 
general, a script starts executing starting with the first instruction in the
script file.  Adhering to the syntax rules is still important, but since interpreted
languages are not compiled, syntax errors become runtime errors.  A program
may run fine until its first syntax error at which point it fails.  

There are other ways of compiling and running programs.  Java for example
represents a compromise between compiled and interpreted languages.
Java source code is compiled into Java bytecode which is not actually machine
code that the operating system and hardware can run directly.  Instead, it is
compiled code for a \gls{jvmLabel}.  This allows a developer to write highly
portable code, compile it and it is runnable on any \gls{jvmLabel} on any system
(write-once, compile-once, run-anywhere).  

In general, interpreted languages are slower than compiled languages because
they are being run through another program (the interpreter) instead of
being executed directly by the processor.  Modern tools
have been introduced to solve this problem.  \gls{jitLabel} compilers have
been developed that take scripts that are not usually compiled, and compile
them to a native machine code format which has the potential to run 
much faster than when interpreted.  Modern web browsers typically do this
for JavaScript code (Chrome's V8 JavaScript engine for example).

Transpilers are source-to-source compilers.  They don't produce assembly
or machine code, instead they take one language and translate it to
another language.  This is sometimes done to ensure that scripting languages
like JavaScript are backwards compatible with previous versions of the language.
Transpilers can also be used to translate one language into the same language
but with different aspects (such as parallel or synchronized code) automatically
added.  They can also be used to translate older languages such as Pascal
to more modern languages as a first step in updating a legacy system.

\clearpage 

\section{Syntax Rules \& Pseudocode}

Programming languages are a lot like spoken languages in that they have 
\emph{syntax} rules.  These rules dictate the appropriate arrangements of words,
punctuation, and other symbols that form valid statements in the language.
For example, many programming languages, commands or statements are terminated
by semicolons (just as most sentences are terminated with a period).  
This is an example of ``punctuation'' a programming language.  

In general, individual executable commands are written one per line.  When 
a program executes, each command executes one after the other.  This is
known as sequential control flow.

A \emph{block} of code is a section of code that has been logically grouped
together.  Many languages allow you to define a block by enclosing the
grouped code around opening and closing curly brackets.  Blocks can
be \emph{nested} within each other to form sub-blocks.

Most languages also have reserved words and symbols that have special 
meaning.  For example, many languages assign special meaning to keywords
such as \mintinline{c}{for}, \mintinline{c}{if}, \mintinline{c}{while}, etc. that 
are used to define various \emph{control structures} such as conditionals and
loops.  Special symbols include operators such as \mintinline{c}{+} and 
\mintinline{c}{*} for performing basic arithmetic.

Failure to adhere to the syntax rules of a particular language will 
lead to bugs and programs that fail to compile and/or run.  Natural
languages are very forgiving: we can generally discern what someone 
is trying to say even if they speak in broken English (to a point).  
However, a compiler or interpreter isn't as smart as a human.  Even
a small syntax error will cause a compiler to completely fail to 
understand the code you have written.  Learning a programming
language is a lot like learning a new spoken language (but, fortunately
a lot easier).  

In subsequent parts of this book we focus on particular languages.  However,
in order to focus on concepts, we'll avoid specific syntax rules by using
\gls{pseudocode}, informal, high-level descriptions of algorithms and 
processes.  Good pseudocode makes use of plain English and mathematical
notation, making it more readable and abstract.  A small example can 
be found in Algorithm \ref{algo:examplePseudocode}.

\begin{algorithm}
\Input{A collection of numbers, $A = \{a_1, a_2, \ldots, a_n\}$}
\Output{The minimal element in $A$}
Let $min$ be equal to $a_1$ \;
\ForEach{element $a_i$ in $A$}{
  \If{$a_i < min$}{
    $a_i$ is less than the smallest element we've found so far \;
    Update $min$ to be equal to $a_i$ \;
  }
}
output $min$ \;
\caption{An example of pseudocode: finding a minimum value}
\label{algo:examplePseudocode}
\end{algorithm}

\section{Documentation, Comments, and Coding Style}

Good code is not just functional, its also beautiful.  Good code
is organized, easy to read, and well documented.  Organization
can be achieved by separating code into useful functions and
collecting functions into \emph{modules} or libraries.  Good
organization means that at any one time, we only need to focus
on a small part of a program.  

It would be difficult to read an essay that contained
random line breaks, paragraphs were not indented, it contained
different spacing or different fonts, etc.  Likewise, code should be 
legible.  Well written code is consistent and makes good use
of whitespace and indentation.  Code in the same code block
should be indented at the same level.  Nested blocks should
be further indented just like the outline of an essay or table
of contents.

Code should be well-documented.  The code itself should be
clear enough that it tells the user \emph{what} the code does
and \emph{how} it does it.  This is called ``self-documenting''
code.  In addition, well-written code should contain sufficient and
clear \emph{comments}.  A comment in a program is intended for
a human user to read.  A comment is ultimately ignored and
has no effect on the actual program.  Good comments tell the
user \emph{why} the code was written or why it was written
the way it was.  Comments provide a high-level description of
what a block of code, function, or program does.  If the 
particular method or algorithm is of interest, it should also be
documented.  

There are typically two ways to write comments.  Single line
comments usually begin with two forward slashes, \mintinline{text}{//}.
Everything after the slashes until the next line is ignored 
by the program.  Multiline comments begin with a \mintinline{text}{/*}
and end with a \mintinline{text}{*/}; everything between them 
is ignored even if it spans multiple lines.  This syntax is shared
among many languages including C, Java, PHP and others.
Some examples:

\begin{minted}{c}
double x = sqrt(y); //this is a single line comment

/*
  This is a multiline comment
  each line is ignored, but allows
  for better formatting
*/

/**
 * This is a doc-style comment, usually placed in
 * front of major portions of code such as a function
 * to provide documentation
 * It begins with a forward-slash-star-star
 */
\end{minted}

The last example above is a doc-style comment.  It originated
with Java, but has since been adopted by many other programming
languages.  Syntactically it is a normal multiline comment, but
begins with a \mintinline{text}{/**}.  Asterisks are aligned together
on each line.  Certain commenting systems allow you to place
other marked up data inside these comments such as labeling
parameters (\mintinline{text}{@param x}) or use HTML code
to provide links.  These doc-style comments are used to provide
documentation for major parts of the code especially functions
and data structures.  Though not part of the language, other
documentation tools can be used to gather the information in
doc-style comments to produce documentation documents (such
as web pages).

Comments should not be trivial: they should not explain something
that should be readily apparent to an experienced user or
programmer.  For example, if a piece of code adds two
numbers together and stores the result, there should not
be a comment that explains the process.  It is a simple and
common enough operation that is self-evident.  However,
if a function uses a particular process or algorithm such
as a Fourier Transform to perform an operation, it would
be appropriate to document it in a series of comments.

Comments can also detail how a function or piece of code
should be used.  This is typically done when developing an
\gls{apiLabel} for use by other programmers.  The API's
available functions should be well-documented so that users
will know how and when to use a particular function.  It
can document the function's expectations and behavior such
as how it handles bad input or error situations.

